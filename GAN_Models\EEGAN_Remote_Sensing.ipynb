{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEGAN - Remote Sensing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO503PfgD6alr99qi6sXrdo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0xtaha/image-super-resloution-for-remote-sensing/blob/main/GAN_Models%5CEEGAN_Remote_Sensing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPmjM70Td5Nk"
      },
      "source": [
        "## 1. train the VGG-19 model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyG-csZfeRB2"
      },
      "source": [
        "### A . Download The Imagenet dataset required to train VGG-19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WNIZIVEek6c"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import tarfile\n",
        "import re\n",
        "import sqlite3\n",
        "\n",
        "def download_urls():\n",
        "    lists = [\n",
        "        ['http://image-net.org/imagenet_data/urls/imagenet_fall11_urls.tgz',\n",
        "         'data/urls/imagenet_fall11_urls.tgz',\n",
        "         'data/urls/fall11.txt'],\n",
        "        ['http://image-net.org/imagenet_data/urls/imagenet_winter11_urls.tgz',\n",
        "         'data/urls/imagenet_winter11_urls.tgz',\n",
        "         'winter11.txt'],\n",
        "        ['http://image-net.org/imagenet_data/urls/imagenet_spring10_urls.tgz',\n",
        "         'data/urls/imagenet_spring10_urls.tgz',\n",
        "         'data/urls/spring10.txt'],\n",
        "        ['http://image-net.org/imagenet_data/urls/imagenet_fall09_urls.tgz',\n",
        "         'data/urls/imagenet_fall09_urls.tgz',\n",
        "         'data/urls/fall09.txt']]\n",
        "\n",
        "    for list_ in lists:\n",
        "        url = list_[0]\n",
        "        tar_path = list_[1]\n",
        "        txt_path = list_[2]\n",
        "\n",
        "        file_size = int(requests.head(url).headers[\"content-length\"])\n",
        "        r = requests.get(url, stream=True)\n",
        "        pbar = tqdm(total=file_size, unit=\"b\", unit_scale=True)\n",
        "        with open(tar_path, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=1024):\n",
        "                f.write(chunk)\n",
        "                pbar.update(len(chunk))\n",
        "        pbar.close()\n",
        "        \n",
        "        tar = tarfile.open(tar_path, 'r')\n",
        "        for item in tar:\n",
        "            tar.extract(item, '.')\n",
        "            shutil.move(item.name, txt_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brb80Qv1er_N"
      },
      "source": [
        "os.mkdir('data')\n",
        "os.mkdir('data/urls')\n",
        "download_urls()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoRyumzde1yF"
      },
      "source": [
        "### B . Create Datebase required for imagenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkq0blQwfAmx"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import sqlite3\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "sys.path.append('./labels')\n",
        "import objects\n",
        "\n",
        "def create_db():\n",
        "    lists = [\n",
        "        ['urls/fall11.txt', 'fall11'],\n",
        "        ['urls/winter11.txt', 'winter11'],\n",
        "        ['urls/spring10.txt', 'spring10'],\n",
        "        ['urls/fall09.txt', 'fall09'],\n",
        "    ]\n",
        "    target = list(objects.objects.keys())\n",
        "        \n",
        "    con = sqlite3.connect('data/imagenet.db')\n",
        "    sql = '''CREATE TABLE urls (id integer primary key autoincrement,\n",
        "        parent varchar(255), object varchar(255), seq varchar(255),\n",
        "        url varchar(65535) unique, download boolean, error boolean);'''\n",
        "    con.execute(sql)\n",
        "    \n",
        "    pattern = r\"(.+)_(.+)\\t(.+)\\n\"\n",
        "    for l in lists:\n",
        "        txt_path, parent = l\n",
        "        print(parent)\n",
        "        with open(txt_path, 'rb') as f:\n",
        "            x = f.readlines()\n",
        "        for i, x_ in tqdm(enumerate(x)):\n",
        "            try: \n",
        "                x_ = str(x_, 'utf-8')\n",
        "            except:\n",
        "                continue\n",
        "            matchOB = re.match(pattern, x_)\n",
        "            object_ = matchOB.group(1)\n",
        "            seq = matchOB.group(2)\n",
        "            url = matchOB.group(3)\n",
        "            if object_ not in target:\n",
        "                continue\n",
        "            sql = '''INSERT INTO urls (parent, object, seq, url, \n",
        "                     download, error) values (?, ?, ?, ?, ?, ?);'''\n",
        "            user = (parent, object_, seq, url, 0, 0)\n",
        "            try:\n",
        "                con.execute(sql, user)\n",
        "            except:\n",
        "                continue\n",
        "        con.commit()\n",
        "    con.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVGzXWWufS1j"
      },
      "source": [
        "create_db()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HogZ_Yp7fZGA"
      },
      "source": [
        "### C . Download the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K130jqcifrUc"
      },
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "import threading\n",
        "import time\n",
        "import sqlite3\n",
        "\n",
        "def download(*h):\n",
        "    def add_error(id_):\n",
        "        sql = '''UPDATE urls SET error = 1 WHERE id = ?;'''\n",
        "        update_status(sql, id_)\n",
        "\n",
        "    def add_download(id_):\n",
        "        sql = '''UPDATE urls SET download = 1 WHERE id = ?;'''\n",
        "        update_status(sql, id_)\n",
        "\n",
        "    def update_status(sql, id_):\n",
        "        con = sqlite3.connect('data/imagenet.db')\n",
        "        con.execute(sql, (str(id_),))\n",
        "        con.commit()\n",
        "        con.close()\n",
        "        \n",
        "    for h_ in h:\n",
        "        id_, parent, object_, seq, url, _, _ = h_\n",
        "        time.sleep(10)\n",
        "        try:\n",
        "            r = requests.get(url, stream=True, timeout=10)\n",
        "        except:\n",
        "            print('ERROR')\n",
        "            add_error(id_)\n",
        "            continue\n",
        "        if r.status_code == 200:\n",
        "            dir_ = os.path.join('data', 'raw', object_)\n",
        "            if not os.path.exists(dir_):\n",
        "                os.mkdir(dir_)\n",
        "            path = os.path.join(dir_, '{}_{}.jpg'.format(parent, seq))\n",
        "            with open(path, 'wb') as f:\n",
        "                try:\n",
        "                    for chunk in r.iter_content(chunk_size=1024):\n",
        "                        f.write(chunk)\n",
        "                except:\n",
        "                    print('ERROR')\n",
        "                    add_error(id_)\n",
        "                    continue\n",
        "            add_download(id_)\n",
        "            print(url)\n",
        "        else:\n",
        "            print('ERROR')\n",
        "            add_error(id_)\n",
        "\n",
        "\n",
        "def get_lists():\n",
        "    con = sqlite3.connect('data/imagenet.db')\n",
        "    cur = con.cursor()\n",
        "    sql = '''SELECT * from urls WHERE download = 0 and error = 0;'''\n",
        "    cur.execute(sql) \n",
        "    lists = cur.fetchall()\n",
        "    cur.close()\n",
        "    con.close()\n",
        "    return lists\n",
        "    \n",
        "\n",
        "def missing_teddy():\n",
        "    ''' \"n04399382: teddy, teddy bear\" cannot be downloaded. '''\n",
        "    ''' There is no n04399382 image. '''\n",
        "    if not os.path.exists('data/raw/n04399382'):\n",
        "    \tos.mkdir('data/raw/n04399382') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOuh1ZWuf1qX"
      },
      "source": [
        "if not os.path.exists('data/raw'):\n",
        "        os.mkdir('data/raw')\n",
        "    \n",
        "n_threads = 3\n",
        "lists = get_lists()\n",
        "x = int(np.ceil(len(lists) / n_threads))\n",
        "\n",
        "for i in range(n_threads):\n",
        "  h = lists[i*x:(i+1)*x]\n",
        "  th = threading.Thread(target=download, args=h).start()\n",
        "\n",
        "missing_teddy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLeq10cGgByw"
      },
      "source": [
        "### D . Preprocess Imagenet images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYbOayC_gURw"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "def preprocess():\n",
        "    pp = glob.glob('data/raw/*')\n",
        "    pp.sort()\n",
        "    for i, p in enumerate(pp):\n",
        "        print(i, p)\n",
        "        paths = glob.glob(os.path.join(p, '*'))\n",
        "        x = []\n",
        "        for path in paths:\n",
        "            with open(path, 'rb') as img_bin:\n",
        "                buff = io.BytesIO()\n",
        "                buff.write(img_bin.read())\n",
        "                buff.seek(0)\n",
        "                try:\n",
        "                    temp = np.array(Image.open(buff), dtype=np.uint8)\n",
        "                except:\n",
        "                    continue\n",
        "                if temp.ndim != 3:\n",
        "                    continue\n",
        "                try:\n",
        "                    img = cv2.cvtColor(temp, cv2.COLOR_RGB2BGR)\n",
        "                except:\n",
        "                    continue\n",
        "            if img is None:\n",
        "                continue\n",
        "            img = cv2.resize(img, (96, 96))\n",
        "            x.append(img)\n",
        "        x = np.array(x, dtype=np.uint8)\n",
        "        np.random.shuffle(x)\n",
        "        r = int(len(x) * 0.95)\n",
        "        x_train = x[:r]\n",
        "        x_test = x[r:]\n",
        "        print(x_train.shape, x_test.shape)\n",
        "        id_ = \"{0:04d}\".format(i)\n",
        "        np.save('data/npy/train/{}.npy'.format(id_), x_train)\n",
        "        np.save('data/npy/test/{}.npy'.format(id_), x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLzI2nTLggER"
      },
      "source": [
        "os.mkdir('data/npy')\n",
        "os.mkdir('data/npy/train')\n",
        "os.mkdir('data/npy/test')\n",
        "preprocess()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iOmdTzUgml0"
      },
      "source": [
        "### E . Train the VGG-19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smYxzY1qiKOt"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "def _load(src):\n",
        "    paths = glob.glob(src)\n",
        "    paths.sort()\n",
        "    paths = paths[:100] # 100 classes\n",
        "    x = None\n",
        "    t = []\n",
        "    for path in tqdm(paths):\n",
        "        id_ = int(os.path.basename(path).split('.')[0])\n",
        "        c = np.load(path)\n",
        "        if c.size == 0:\n",
        "            continue\n",
        "        l = [id_ for _ in range(c.shape[0])]\n",
        "        if x is None:\n",
        "            x = c\n",
        "        else:\n",
        "            x = np.concatenate((x, c), 0)\n",
        "        t += l\n",
        "    t = np.array(t)\n",
        "    return [x, t]\n",
        "\n",
        "def load():\n",
        "    x_train, t_train = _load('./imagenet/data/npy/train/*')\n",
        "    x_test, t_test = _load('./imagenet/data/npy/test/*')\n",
        "    return x_train, t_train, x_test, t_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWyJQ_EUguGG"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import sys\n",
        "sys.path.append('../utils')\n",
        "from vgg19 import VGG19\n",
        "import load\n",
        "import augment\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch_size = 128\n",
        "\n",
        "def train():\n",
        "    x = tf.placeholder(tf.float32, [None, 96, 96, 3])\n",
        "    t = tf.placeholder(tf.int32, [None])\n",
        "    is_training = tf.placeholder(tf.bool, [])\n",
        "\n",
        "    model = VGG19(x, t, is_training)\n",
        "    sess = tf.Session()\n",
        "    with tf.variable_scope('vgg19'):\n",
        "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "    opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    train_op = opt.minimize(model.loss, global_step=global_step)\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "\n",
        "    # Restore the latest model\n",
        "    if tf.train.get_checkpoint_state('backup/'):\n",
        "        saver = tf.train.Saver()\n",
        "        saver.restore(sess, 'backup/latest')\n",
        "\n",
        "    # Load the dataset\n",
        "    x_train, t_train, x_test, t_test = load()\n",
        "\n",
        "    # Train\n",
        "    while True:\n",
        "        epoch = int(sess.run(global_step) / np.ceil(len(x_train)/batch_size)) + 1\n",
        "        print('epoch:', epoch)\n",
        "        perm = np.random.permutation(len(x_train))\n",
        "        x_train = x_train[perm]\n",
        "        t_train = t_train[perm]\n",
        "        sum_loss_value = 0\n",
        "        for i in tqdm(range(0, len(x_train), batch_size)):\n",
        "            x_batch = augment.augment(x_train[i:i+batch_size])\n",
        "            t_batch = t_train[i:i+batch_size]\n",
        "            _, loss_value = sess.run(\n",
        "                [train_op, model.loss],\n",
        "                feed_dict={x: x_batch, t: t_batch, is_training: True})\n",
        "            sum_loss_value += loss_value\n",
        "        print('loss:', sum_loss_value)\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        saver.save(sess, 'backup/latest', write_meta_graph=False)\n",
        "\n",
        "        prediction = np.array([])\n",
        "        answer = np.array([])\n",
        "        for i in range(0, len(x_test), batch_size):\n",
        "            x_batch = augment.augment(x_test[i:i+batch_size])\n",
        "            t_batch = t_test[i:i+batch_size]\n",
        "            output = model.out.eval(\n",
        "                feed_dict={x: x_batch, is_training: False}, session=sess)\n",
        "            prediction = np.concatenate([prediction, np.argmax(output, 1)])\n",
        "            answer = np.concatenate([answer, t_batch])\n",
        "            correct_prediction = np.equal(prediction, answer)\n",
        "        accuracy = np.mean(correct_prediction)\n",
        "        print('accuracy:', accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzt5ztHChaab"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EON8D7yrhicx"
      },
      "source": [
        "## 2 . Download the Pretrained VGG-19 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHlS2G9nh1N8"
      },
      "source": [
        "# find the share link of the file/folder on Google Drive\n",
        "file_share_link = \"https://drive.google.com/open?id=0B-s6ok7B0V9vcXNfSzdjZ0lCc0k\"\n",
        "\n",
        "# extract the ID of the file\n",
        "file_id = file_share_link[file_share_link.find(\"=\") + 1:]\n",
        "\n",
        "# append the id to this REST command\n",
        "file_download_link = \"https://docs.google.com/uc?export=download&id=\" + file_id "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEJXYJy5kMBI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}